{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects2/aisd/2024/seongcho/miniconda3/envs/sae/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "from sae_lens import SAE\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f56b0f306b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "    sae_id=\"layer_20/width_16k/canonical\",\n",
    ")\n",
    "sae = sae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(policy_net, observation):\n",
    "    # Get activated logits from policy_net (B, dict_size)\n",
    "    logits = policy_net(observation)\n",
    "    # Add small random noise for diversity\n",
    "    epsilon = torch.randn_like(logits) * 0.01\n",
    "    logits_noisy = logits + epsilon\n",
    "    # Select top-1 value and index from noisy logits\n",
    "    topk_vals, topk_indices = torch.topk(logits_noisy, k=1, dim=-1)\n",
    "    # Clamp top values to a minimum of 1 (values below 1 become 1)\n",
    "    topk_vals = torch.clamp(topk_vals, min=50.0)\n",
    "    # Create one-hot action vector with the clamped top value at the selected index\n",
    "    action = torch.zeros_like(logits)\n",
    "    action.scatter_(1, topk_indices, topk_vals)\n",
    "    # Compute softmax probabilities from the original logits and get log probability of the chosen index\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    chosen_probs = torch.gather(probs, 1, topk_indices)\n",
    "    log_prob = torch.log(chosen_probs).squeeze(1)\n",
    "    return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim, dict_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.linear = nn.Linear(latent_dim, dict_size)\n",
    "        self.activation = nn.Tanh()  # using Tanh instead of ReLU\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        logits = self.linear(obs)\n",
    "        activated = self.activation(logits)\n",
    "        return activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.act = nn.Tanh()  # using Tanh here\n",
    "        self.fc2 = nn.Linear(latent_dim, 1)\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        x = self.act(self.fc1(obs))\n",
    "        value = self.fc2(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_steering_hook(policy_net, sae):\n",
    "    class SteeringHook:\n",
    "        def __init__(self, policy_net, sae):\n",
    "            self.policy_net = policy_net\n",
    "            self.sae = sae\n",
    "            self.observation = None   # will be tensor of shape (B, latent_dim)\n",
    "            self.action = None        # (B,)\n",
    "            self.log_prob = None      # (B,)\n",
    "\n",
    "        def __call__(self, module, inputs):\n",
    "            residual = inputs[0]  # shape: (B, seq_len, hidden_dim)\n",
    "            observation = residual[:, -1, :]  # (B, latent_dim)\n",
    "            self.observation = observation.detach()\n",
    "            action, log_prob = select_action(self.policy_net, observation)\n",
    "            steering = sae.decode(action)  # shape: (B, hidden_dim)\n",
    "            self.action = action.detach()\n",
    "            self.log_prob = log_prob.detach()\n",
    "            # Add the corresponding steering vector to the last token.\n",
    "            residual[:, -1, :] = residual[:, -1, :] + steering\n",
    "            return (residual)\n",
    "    return SteeringHook(policy_net, sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer:\n",
    "    def __init__(self, policy, critic, batch_size=8, ppo_clip=0.2, lr=1e-4):\n",
    "        self.policy = policy\n",
    "        self.critic = critic\n",
    "        self.batch_size = batch_size\n",
    "        self.ppo_clip = ppo_clip\n",
    "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "    \n",
    "    def compute_advantages(self, rewards, values):\n",
    "        return rewards - values\n",
    "\n",
    "    def train_step(self, observations, actions, rewards, old_log_probs):\n",
    "        # Compute policy outputs.\n",
    "        mean = self.policy(observations)  # shape: (B, action_dim)\n",
    "        sigma = torch.ones_like(mean) * 0.1  # fixed sigma\n",
    "        dist = torch.distributions.Normal(mean, sigma)\n",
    "        new_log_probs = dist.log_prob(actions).sum(dim=-1)\n",
    "        \n",
    "        # Compute critic outputs.\n",
    "        values = self.critic(observations).squeeze(-1)\n",
    "        \n",
    "        advantages = rewards - values  # simple advantage\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.ppo_clip, 1.0 + self.ppo_clip) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        critic_loss = nn.MSELoss()(values, rewards)\n",
    "        \n",
    "        # Combine losses to perform a single backward pass.\n",
    "        total_loss = policy_loss + critic_loss\n",
    "        \n",
    "        self.optimizer_policy.zero_grad()\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer_policy.step()\n",
    "        self.optimizer_critic.step()\n",
    "        \n",
    "        return policy_loss.item(), critic_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMLUDataLoader:\n",
    "    def __init__(self, dataset, split, limit=None):\n",
    "        self.data = dataset[split]\n",
    "        if limit is not None:\n",
    "            self.data = self.data.select(range(limit))\n",
    "        self.index = 0\n",
    "        self.n_samples = len(self.data)\n",
    "        \n",
    "    def get_batch(self, batch_size):\n",
    "        batch = []\n",
    "        for _ in range(batch_size):\n",
    "            sample = self.data[self.index % self.n_samples]\n",
    "            self.index += 1\n",
    "            batch.append({\n",
    "                \"question\": sample[\"question\"],\n",
    "                \"choices\": sample[\"choices\"],\n",
    "                \"answer\": sample[\"answer\"]\n",
    "            })\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
    "train_loader = MMLUDataLoader(mmlu_dataset, split=\"auxiliary_train\")\n",
    "val_loader = MMLUDataLoader(mmlu_dataset, split=\"validation\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 2304   # Gemma's latent dimension (from layer 20)\n",
    "DICT_SIZE = 16384   # SAE dictionary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/projects2/aisd/2024/seongcho/miniconda3/envs/sae/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    (rotary_emb): Gemma2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", output_hidden_states=True).to(device)\n",
    "llm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork(LATENT_DIM, DICT_SIZE).to(device)\n",
    "critic_net = CriticNetwork(LATENT_DIM).to(device)\n",
    "ppo_trainer = PPOTrainer(policy_net, critic_net, batch_size=8, ppo_clip=0.2, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def train(num_steps=20000, validate_every=100, checkpoint_every=200, checkpoint_dir=\"./checkpoints\"):\n",
    "    wandb.init(project=\"gemma_mmlu_ppo\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    best_val_accuracy = 0.0\n",
    "    train_acc_sum = 0.0\n",
    "    train_acc_count = 0\n",
    "    for step in tqdm(range(1, num_steps + 1), desc=\"Training Steps\"):\n",
    "        batch = train_loader.get_batch(ppo_trainer.batch_size)\n",
    "        prompts, correct_answers = [], []\n",
    "        for sample in batch:\n",
    "            question = sample[\"question\"]\n",
    "            choices = sample[\"choices\"]\n",
    "            if isinstance(sample[\"answer\"], int):\n",
    "                correct_answer = chr(65 + sample[\"answer\"])\n",
    "            else:\n",
    "                correct_answer = sample[\"answer\"].strip().upper()\n",
    "            prompt = (question + \"\\n\" +\n",
    "                      \"\\n\".join(f\"{chr(65+i)}. {choice}\" for i, choice in enumerate(choices)) +\n",
    "                      \"\\nChoose one of the following options only: A, B, C, or D\" +\n",
    "                      \"\\nAnswer:\")\n",
    "            prompts.append(prompt)\n",
    "            correct_answers.append(correct_answer)\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        steering_hook = batch_steering_hook(policy_net, sae)\n",
    "        hook_handle = llm.model.layers[20].register_forward_pre_hook(steering_hook)\n",
    "        generated_ids = llm.generate(inputs, max_new_tokens=1)\n",
    "        hook_handle.remove()\n",
    "        batch_rewards = []\n",
    "        for i in range(generated_ids.shape[0]):\n",
    "            gen_tok = tokenizer.decode(generated_ids[i, -1]).strip()\n",
    "            predicted_label = gen_tok[0].upper() if gen_tok else \"\"\n",
    "            reward = 1 if predicted_label == correct_answers[i] else 0\n",
    "            batch_rewards.append(reward)\n",
    "        obs_batch = steering_hook.observation\n",
    "        action_batch = steering_hook.action\n",
    "        log_prob_batch = steering_hook.log_prob\n",
    "        rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "        nonzero_vals = action_batch[action_batch != 0]\n",
    "        avg_activation = nonzero_vals.mean().item() if nonzero_vals.numel() > 0 else 0.0\n",
    "        policy_loss, critic_loss = ppo_trainer.train_step(obs_batch, action_batch, rewards_tensor, log_prob_batch)\n",
    "        train_accuracy = sum(batch_rewards) / len(batch_rewards)\n",
    "        train_acc_sum += train_accuracy\n",
    "        train_acc_count += 1\n",
    "        wandb.log({\"step\": step, \"policy_loss\": policy_loss, \"critic_loss\": critic_loss, \n",
    "                   \"train_accuracy\": train_accuracy, \"avg_activation\": avg_activation})\n",
    "        if step % validate_every == 0:\n",
    "            used_indices = set()  # clear used_indices each validation step\n",
    "            val_batch = val_loader.get_batch(val_loader.n_samples)\n",
    "            val_prompts, correct_answers_val = [], []\n",
    "            for sample in val_batch:\n",
    "                question = sample[\"question\"]\n",
    "                choices = sample[\"choices\"]\n",
    "                if isinstance(sample[\"answer\"], int):\n",
    "                    correct_answer = chr(65 + sample[\"answer\"])\n",
    "                else:\n",
    "                    correct_answer = sample[\"answer\"].strip().upper()\n",
    "                prompt = (question + \"\\n\" +\n",
    "                          \"\\n\".join(f\"{chr(65+i)}. {choice}\" for i, choice in enumerate(choices)) +\n",
    "                          \"\\nChoose one of the following options only: A, B, C, or D\" +\n",
    "                          \"\\nAnswer:\")\n",
    "                val_prompts.append(prompt)\n",
    "                correct_answers_val.append(correct_answer)\n",
    "            inputs_val = tokenizer(val_prompts, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "            steering_hook_val = batch_steering_hook(policy_net, sae)\n",
    "            hook_handle_val = llm.model.layers[20].register_forward_pre_hook(steering_hook_val)\n",
    "            generated_ids_val = llm.generate(inputs_val, max_new_tokens=1)\n",
    "            hook_handle_val.remove()\n",
    "            total_correct = 0\n",
    "            val_action_batch = steering_hook_val.action\n",
    "            topk_vals_val, topk_indices_val = torch.topk(val_action_batch, k=1, dim=-1)\n",
    "            for idx in topk_indices_val.view(-1).tolist():\n",
    "                used_indices.add(idx)\n",
    "            for i in range(generated_ids_val.shape[0]):\n",
    "                gen_tok = tokenizer.decode(generated_ids_val[i, -1]).strip()\n",
    "                predicted_label = gen_tok[0].upper() if gen_tok else \"\"\n",
    "                if predicted_label == correct_answers_val[i]:\n",
    "                    total_correct += 1\n",
    "            val_accuracy = total_correct / len(val_batch)\n",
    "            avg_train_accuracy = train_acc_sum / train_acc_count if train_acc_count > 0 else 0\n",
    "            wandb.log({\"step\": step, \"val_accuracy\": val_accuracy, \"avg_train_accuracy\": avg_train_accuracy, \n",
    "                       \"unique_indices\": len(used_indices)})\n",
    "            print(f\"Step {step}: Policy Loss {policy_loss:.4f}, Critic Loss {critic_loss:.4f}, \"\n",
    "                  f\"Avg Train Acc {avg_train_accuracy:.4f}, Val Acc {val_accuracy:.4f}, \"\n",
    "                  f\"Unique Indices: {len(used_indices)}\")\n",
    "            train_acc_sum, train_acc_count = 0.0, 0\n",
    "        if step % checkpoint_every == 0:\n",
    "            checkpoint_path = os.path.join(\n",
    "                checkpoint_dir,\n",
    "                f\"gemma-2-2b_layer20_ppo_lr{ppo_trainer.optimizer_policy.defaults['lr']}_batch{ppo_trainer.batch_size}_step{step}.pt\"\n",
    "            )\n",
    "            torch.save({\n",
    "                'step': step,\n",
    "                'policy_state_dict': policy_net.state_dict(),\n",
    "                'critic_state_dict': critic_net.state_dict(),\n",
    "                'optimizer_policy_state_dict': ppo_trainer.optimizer_policy.state_dict(),\n",
    "                'optimizer_critic_state_dict': ppo_trainer.optimizer_critic.state_dict()\n",
    "            }, checkpoint_path)\n",
    "            if step % validate_every == 0 and val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_checkpoint_path = os.path.join(checkpoint_dir, \"best_policy.pt\")\n",
    "                torch.save({\n",
    "                    'step': step,\n",
    "                    'policy_state_dict': policy_net.state_dict(),\n",
    "                    'critic_state_dict': critic_net.state_dict(),\n",
    "                    'optimizer_policy_state_dict': ppo_trainer.optimizer_policy.state_dict(),\n",
    "                    'optimizer_critic_state_dict': ppo_trainer.optimizer_critic.state_dict()\n",
    "                }, best_checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseonglae\u001b[0m (\u001b[33mtexonom\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cs/student/projects2/aisd/2024/seongcho/wandb/run-20250228_120829-ktbd5kt8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/texonom/gemma_mmlu_ppo/runs/ktbd5kt8' target=\"_blank\">silver-blaze-41</a></strong> to <a href='https://wandb.ai/texonom/gemma_mmlu_ppo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/texonom/gemma_mmlu_ppo' target=\"_blank\">https://wandb.ai/texonom/gemma_mmlu_ppo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/texonom/gemma_mmlu_ppo/runs/ktbd5kt8' target=\"_blank\">https://wandb.ai/texonom/gemma_mmlu_ppo/runs/ktbd5kt8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   0%|          | 0/20000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n",
      "Training Steps:   0%|          | 100/20000 [01:17<9:04:38,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: Policy Loss 0.0409, Critic Loss 0.3624, Avg Train Acc 0.3825, Val Acc 0.4800, Unique Indices: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|          | 199/20000 [02:00<58:08,  5.68it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200: Policy Loss -0.0000, Critic Loss 0.0144, Avg Train Acc 0.6550, Val Acc 0.4900, Unique Indices: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   2%|▏         | 300/20000 [02:38<6:23:43,  1.17s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300: Policy Loss 0.1686, Critic Loss 0.1412, Avg Train Acc 0.7300, Val Acc 0.4800, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   2%|▏         | 399/20000 [03:18<3:31:29,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400: Policy Loss 0.0563, Critic Loss 0.0481, Avg Train Acc 0.7937, Val Acc 0.4800, Unique Indices: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   2%|▎         | 500/20000 [04:33<8:29:47,  1.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: Policy Loss -0.0000, Critic Loss 0.0733, Avg Train Acc 0.8938, Val Acc 0.4800, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   3%|▎         | 599/20000 [05:12<1:08:47,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600: Policy Loss -0.0000, Critic Loss 0.0716, Avg Train Acc 0.8612, Val Acc 0.5000, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   4%|▎         | 700/20000 [06:58<11:13:19,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 700: Policy Loss 0.0097, Critic Loss 0.3348, Avg Train Acc 0.7013, Val Acc 0.4800, Unique Indices: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   4%|▍         | 799/20000 [08:34<4:56:35,  1.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 800: Policy Loss 0.0429, Critic Loss 0.0936, Avg Train Acc 0.7100, Val Acc 0.4900, Unique Indices: 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   4%|▍         | 900/20000 [10:17<9:48:30,  1.85s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 900: Policy Loss 0.1717, Critic Loss 0.1383, Avg Train Acc 0.7025, Val Acc 0.4900, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   5%|▍         | 999/20000 [11:50<4:45:29,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Policy Loss 0.1444, Critic Loss 0.1196, Avg Train Acc 0.7238, Val Acc 0.4900, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   6%|▌         | 1100/20000 [14:17<9:49:11,  1.87s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1100: Policy Loss 0.1838, Critic Loss 0.3074, Avg Train Acc 0.6787, Val Acc 0.4900, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   6%|▌         | 1199/20000 [15:47<5:06:54,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1200: Policy Loss 0.0298, Critic Loss 0.1865, Avg Train Acc 0.7200, Val Acc 0.5100, Unique Indices: 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   6%|▋         | 1300/20000 [17:38<9:51:52,  1.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1300: Policy Loss 0.0414, Critic Loss 0.3279, Avg Train Acc 0.6863, Val Acc 0.5000, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   7%|▋         | 1399/20000 [19:15<5:05:49,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1400: Policy Loss 0.0871, Critic Loss 0.2009, Avg Train Acc 0.6925, Val Acc 0.4900, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   8%|▊         | 1500/20000 [21:04<10:37:24,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500: Policy Loss 0.0243, Critic Loss 0.2891, Avg Train Acc 0.6937, Val Acc 0.5000, Unique Indices: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   8%|▊         | 1599/20000 [22:41<5:16:15,  1.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1600: Policy Loss 0.2449, Critic Loss 0.2176, Avg Train Acc 0.6875, Val Acc 0.5000, Unique Indices: 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   8%|▊         | 1700/20000 [24:38<9:55:42,  1.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1700: Policy Loss 0.1278, Critic Loss 0.2253, Avg Train Acc 0.6987, Val Acc 0.4900, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   9%|▉         | 1799/20000 [26:18<5:10:42,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1800: Policy Loss 0.0486, Critic Loss 0.2593, Avg Train Acc 0.7050, Val Acc 0.5000, Unique Indices: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  10%|▉         | 1900/20000 [28:06<9:33:34,  1.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1900: Policy Loss 0.0580, Critic Loss 0.0870, Avg Train Acc 0.7113, Val Acc 0.4900, Unique Indices: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  10%|▉         | 1999/20000 [29:42<5:04:17,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000: Policy Loss 0.1487, Critic Loss 0.1989, Avg Train Acc 0.6975, Val Acc 0.4800, Unique Indices: 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  10%|█         | 2100/20000 [31:42<9:20:54,  1.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2100: Policy Loss 0.1816, Critic Loss 0.1425, Avg Train Acc 0.7150, Val Acc 0.5000, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  11%|█         | 2199/20000 [33:20<4:41:21,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2200: Policy Loss 0.0984, Critic Loss 0.1764, Avg Train Acc 0.7075, Val Acc 0.5100, Unique Indices: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  12%|█▏        | 2300/20000 [35:13<9:10:36,  1.87s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2300: Policy Loss 0.1493, Critic Loss 0.2851, Avg Train Acc 0.7050, Val Acc 0.4800, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  12%|█▏        | 2399/20000 [36:50<7:32:34,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2400: Policy Loss 0.1623, Critic Loss 0.1573, Avg Train Acc 0.6700, Val Acc 0.4900, Unique Indices: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  12%|█▎        | 2500/20000 [38:36<10:41:23,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500: Policy Loss 0.0960, Critic Loss 0.1678, Avg Train Acc 0.6925, Val Acc 0.5200, Unique Indices: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  13%|█▎        | 2599/20000 [40:15<5:08:48,  1.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2600: Policy Loss -0.0000, Critic Loss 0.0685, Avg Train Acc 0.6737, Val Acc 0.5000, Unique Indices: 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  14%|█▎        | 2700/20000 [42:05<9:43:53,  2.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2700: Policy Loss 0.0116, Critic Loss 0.0228, Avg Train Acc 0.7075, Val Acc 0.5000, Unique Indices: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  14%|█▍        | 2799/20000 [43:42<4:45:46,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2800: Policy Loss 0.0662, Critic Loss 0.1831, Avg Train Acc 0.6750, Val Acc 0.4900, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  14%|█▍        | 2900/20000 [45:28<10:21:57,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2900: Policy Loss 0.0715, Critic Loss 0.1299, Avg Train Acc 0.7013, Val Acc 0.4800, Unique Indices: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  15%|█▍        | 2999/20000 [46:59<4:56:17,  1.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000: Policy Loss 0.1003, Critic Loss 0.2673, Avg Train Acc 0.6825, Val Acc 0.4700, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  16%|█▌        | 3100/20000 [50:01<9:53:30,  2.11s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3100: Policy Loss 0.2471, Critic Loss 0.2874, Avg Train Acc 0.6963, Val Acc 0.4900, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  16%|█▌        | 3199/20000 [51:36<5:07:56,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3200: Policy Loss 0.1095, Critic Loss 0.1565, Avg Train Acc 0.7200, Val Acc 0.5000, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  16%|█▋        | 3300/20000 [53:27<9:15:07,  1.99s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3300: Policy Loss 0.0634, Critic Loss 0.1897, Avg Train Acc 0.6663, Val Acc 0.5000, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  17%|█▋        | 3399/20000 [55:01<4:13:40,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3400: Policy Loss 0.1591, Critic Loss 0.1481, Avg Train Acc 0.6600, Val Acc 0.4900, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  18%|█▊        | 3500/20000 [56:50<8:52:32,  1.94s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3500: Policy Loss 0.1417, Critic Loss 0.3130, Avg Train Acc 0.7050, Val Acc 0.4900, Unique Indices: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  18%|█▊        | 3599/20000 [58:25<4:55:25,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3600: Policy Loss 0.1367, Critic Loss 0.3177, Avg Train Acc 0.6637, Val Acc 0.4800, Unique Indices: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  18%|█▊        | 3700/20000 [1:00:13<8:45:22,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3700: Policy Loss 0.2872, Critic Loss 0.3500, Avg Train Acc 0.6863, Val Acc 0.4900, Unique Indices: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:  19%|█▉        | 3768/20000 [1:01:17<4:05:14,  1.10it/s]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
